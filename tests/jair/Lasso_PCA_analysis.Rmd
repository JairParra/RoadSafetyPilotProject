---
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{amsthm}
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
  - \fancyhead[CO,CE]{Hair Parra}
  - \fancyfoot[CO,CE]{Notes by Hair Parra}
  - \fancyfoot[LE,RO]{\thepage}
title: "Road Safety Pilot Project - EDA"
author: "Hair Albeiro Parra Barrera"
geometry: margin=1.3cm
always_allow_html: true
output: 
    pdf_document: 
      extra_dependencies: ["array", "amsmath","booktabs"]
---

\newtheorem{assumption}{Assumption}[assumption]
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{remark*}{Remark}
\newtheorem{aside*}{Aside}
\newtheorem{exercise*}{Exercise}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=9, fig.height=6) 

# configurations for plot 
my_plot_hook <- function(x, options)
  paste("\n", knitr::hook_plot_tex(x, options), "\n")
knitr::knit_hooks$set(plot = my_plot_hook)
```


## Libraries 

```{r libraries, message=FALSE, warning=FALSE}
# CRAN libraries
library("sf") # temp
library("here")
library("tidyr")
library("dplyr")
library("haven")
library("tibble")
library("readxl")
library("corrplot")
library("factoextra")
library("ggmap") # temp 
library("randomForest")
library("ggplot2")
library("elasticnet")
library("data.table")

# Custom scripts  
source(here("functions", "utils.R"))
source(here("functions", "clean_data.R"))
source(here("functions", "install_packages.R"))

# load packages if necessary 
# f_load_packages()
```


# Data Preprocessing 

```{r}
# Load data from the data_raw folder with UTF-8 encoding
dat <- read.csv(here("data_raw", "data_final.csv"), sep=";")

# perform data cleaning 
dat <- f_clean_data(dat)
head(dat)
```

### List of covariates 

```{r}
# list all categorical covarariates 
categ_vars <- c("all_pedest", "median", "green_stra", "half_phase", "any_ped_pr", 
                "ped_countd", "lt_protect", "lt_restric", "lt_prot_re",
                "any_exclus", "parking", "curb_exten", "all_red_an", "new_half_r",
                "borough")

# list all numerical covariates 
num_vars <- c("fi", "fli", "fri", "fti", "cli",
              "cri", "cti", "acc", "ln_pi", "ln_fi", "ln_fli",
              "ln_fri", "ln_fti", "ln_cli", "ln_cri", "ln_cti",
              "tot_crossw", "number_of_", "avg_crossw", "tot_road_w", 
              "north_veh", "north_ped", "east_veh", "east_ped",
              "south_veh", "south_ped", "west_veh", "west_ped", 
              "total_lane", "of_exclusi", "commercial",
              "distdt", "ln_distdt", "traffic_10000", "ped_100")

# all covariates together 
all_vars <- c(categ_vars, num_vars)

# verify correct which variables were taken out 
length(all_vars)
ncol(dat)
setdiff(colnames(dat), all_vars)
```

```{r}
str(dat)
```
```{r}
# load the Dicionnaire_final object similar to before
varnames <- read_excel(here("data_raw","Dictionnaire_final.xlsx"))
varnames_dict <- setNames(as.list(varnames$DESCRIPTION), varnames$NOM)
names(varnames_dict) <- gsub("[^[:alnum:]_]+", "", names(varnames_dict)) # ensure clean list names
```


# Lasso PCA Factor Analysis

In order to perform regularized Lasso, we first need to standarized the data.

```{r}
# standarize the numeric columns of the data
dat_std <- f_standardize_data(dat, cols=num_vars)
dat_std 
```

## Ordinary PCA

```{r}
# subset only the numerical covariates 
datpca <- dat_std[, num_vars]

# Perform PCA on the standardized data 'datpca'
pca <- prcomp(datpca)

# Get and print the eigenvalues, variance percentages, and cumulative variance percentages
get_eig(pca)
```

```{r}
# Visualize the eigenvalues using a scree plot
fviz_eig(pca)
```
We could use up to the 6th component, explaining around 72% of the variance. 


## Lasso PCA

```{r}
# num of factors to use 
K = 6 

# Perform Sparse PCA on the standardized data 'datpca'
spcafit=spca(datpca,
             K=K, # number of components
             sparse="penalty", # penalty method
             para=rep(60,K)) # values of lambda for each component

spcafit
```

# Random Forest Importance 

$$
\text{%IncMSE}(X_i) = \frac{1}{N} \sum_{t=1}^{N} \left( \text{MSE}_{\text{perturbed}, t} - \text{MSE}_{\text{original}, t} \right)
$$

```{r}
# Load the necessary library
library(randomForest)

# Ensure that 'dat' contains only the variables in 'all_vars' plus the target variable 'acc'
predictors <- all_vars[all_vars != "acc"]
dat_rf <- dat[c("acc", predictors)]

# Train the random forest model
set.seed(123)  # for reproducibility
rf_model <- randomForest(acc ~ ., data = dat_rf, importance = TRUE)

# Extract variable importance
importance_rf <- importance(rf_model)

# Create a data frame of variables and their importance
variable_importance <- data.frame(Variable = rownames(importance_rf), 
                                  Importance = importance_rf[, "%IncMSE"])
variable_importance <- variable_importance[order(variable_importance$Importance, 
                                                 decreasing = TRUE), ]
variable_importance$description <- sapply(variable_importance$Variable, 
                                                function(x) f_get_description(x, varnames_dict))
rownames(variable_importance) <- NULL

# Print the sorted variable importance
print(variable_importance)
```


```{r}
# Select top 20 most important features
top_20_features <- head(variable_importance, 20)

# Create a horizontal barplot of the 20 most important features
ggplot(top_20_features, aes(x = reorder(Variable, Importance), y = Importance, fill = Importance)) +
  geom_bar(stat = "identity") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +  # Apply blue gradient
  coord_flip() +  # Make it horizontal
  labs(title = "Top 20 Most Important Features in Random Forest Model (by %IncMSE)",
       x = "Feature",
       y = "Importance (%IncMSE)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "none")  # Adjust x-axis text

```
















