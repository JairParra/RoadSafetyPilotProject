---
title: "Road Safety Pilot Project - Forecasting"
author: "Hair"
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO,CE]{Hair Parra}
- \fancyfoot[CO,CE]{Notes by Hair Parra}
- \fancyfoot[LE,RO]{\thepage}
output:
  html_document:
    df_print: paged
  pdf_document:
    extra_dependencies:
    - array
    - amsmath
    - booktabs
always_allow_html: true
geometry: margin=1.3cm
---

\newtheorem{assumption}{Assumption}[assumption]
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{remark*}{Remark}
\newtheorem{aside*}{Aside}
\newtheorem{exercise*}{Exercise}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=9, fig.height=6) 

# configurations for plot 
my_plot_hook <- function(x, options)
  paste("\n", knitr::hook_plot_tex(x, options), "\n")
knitr::knit_hooks$set(plot = my_plot_hook)
```

## Libraries 

```{r libraries, message=FALSE, warning=FALSE}
# CRAN libraries
library("sf") # temp
library("here")
library("tidyr")
library("dplyr")
library("tibble")
library("readxl")
library("corrplot")
library("lubridate")
library("randomForest")
library("ggmap") # temp 
library("ggplot2")
library("data.table")
library("MASS")
library("glmnet")
library("caret")
library("forcats")

# Custom scripts  
source(here("functions", "utils.R"))
source(here("functions", "clean_data.R"))
source(here("functions", "install_packages.R"))

# load packages if necessary 
#f_load_packages()
```


## Load the data 

```{r message=FALSE, warning=FALSE}
############################
### 1. Load the Raw Data ###
############################

# Load the dataset and perform data cleaning 
dat_orig <- read.csv(here("data_raw", "data_final.csv"), sep=";")

# store the intersections id_no to rue_1 and rue_2 mapping
inter_names <- subset(dat_orig, select = c(int_no, x, y, rue_1, rue_2))

#########################
### 2. Clean the data ###
#########################

# Create two clean versions of the data 
dat <- f_clean_data(dat_orig, 
                    group_boroughs = TRUE, 
                    drop_borough=TRUE, 
                    drop_year=TRUE, 
                    standarize = TRUE,
                    numerical_categories = FALSE) # integer-based values
# 
# dat_v2 <- f_clean_data(dat_orig, 
#                     group_boroughs = TRUE, 
#                     drop_borough=TRUE, 
#                     drop_year=TRUE, 
#                     standarize = TRUE,
#                     numerical_categories = TRUE) # integer-based values

# Drop the raw data object 
rm(dat_orig); gc(verbose=FALSE);

#################################
### 3. Cleate Dummies Version ###
#################################

## Create an additional version which contains dummies instead of factors 

# All the variables should go into the dummies, except `acc`
all_vars <- setdiff(colnames(dat), "acc")

# Create a version with dummies
dat_dum <- f_convert_to_dummies(dat, all_vars)

# Reassign the target variable
dat_dum$acc <- dat$acc

##################################################
### 4. Extract variable names and descriptions ###
##################################################

# Load variable descriptors 
result <- f_load_varnames(here("data_raw", "Dictionnaire_final.xlsx"))
varnames <- result$varnames
varnames_dict <- result$varnames_dict

# preview dat 
head(dat)
head(dat_dum)
```



## Variable Selection 

In this section to keep it short, we will perform variable selection using the following methods: 

- Stepwise with AIC/BIC 
- Lasso selection
- RF Importance Selection
- Top Spearman-correlated covariates with acc 


### Train-validation split 

In order to perform variable selection, and be able to compare the different methods, we will split the data into a training and validation set. 


```{r}
# Set seed for reproducibility
set.seed(123)

# Split data - example using dat
trainIndex <- createDataPartition(dat$acc, 
                                  p = .8,  # 80% of the data
                                  list = FALSE, 
                                  times = 1)
dat_train <- dat[ trainIndex,]
dat_val   <- dat[-trainIndex,]

# Print dimensions of the dat_train and dat_val
cat("Dimensions of dat_train:", dim(dat_train), "\n")
cat("Dimensions of dat_val:", dim(dat_val), "\n")

# Repeat for dat_dum if necessary
trainIndex_dum <- createDataPartition(dat_dum$acc,
                                      p = .8, 
                                      list = FALSE, 
                                      times = 1)
dat_dum_train <- dat_dum[ trainIndex_dum,]
dat_dum_val   <- dat_dum[-trainIndex_dum,]

# Print dimensions of the dat_dum_train and dat_dum_val
cat("Dimensions of dat_dum_train:", dim(dat_dum_train), "\n")
cat("Dimensions of dat_dum_val:", dim(dat_dum_val), "\n")
```


### Stepwise with AIC/BIC 

A couple of important interactions to consider:

- **Time interactions**: With `month` and day of the week `dow`
- **Traffic Flow and Pedestrian Protection Measures**: Interactions between the average annual daily flow for vehicles and pedestrians (e.g., `fi`, `pi`) and pedestrian protection measures (`any_ped_pr`, `lt_protect`, `lt_restric`, `lt_prot_re`, `ped_countd`, `curb_exten`) could reveal how traffic volume interacts with safety measures.
- **Road Characteristics and Safety Measures:** The presence of medians, exclusive lanes, and the total width of roads (`median`, `any_exclus`, `tot_road_w`) may have different impacts on safety when combined with pedestrian safety measures.
- **Directional Traffic Flow with Specific Safety Measures:** Examining how the flow of traffic in specific directions (e.g., `north_veh`, `east_veh`, `south_veh`, `west_veh`) interacts with pedestrian phases and countdowns could highlight directional risks.
- **Pedestrian and Vehicle Flow Interactions:** The interactions between pedestrian flow (`pi`, `north_ped`, `east_ped`, `south_ped`, `west_ped`) and vehicle flow (`fi`, `north_veh`, `east_veh`, `south_veh`, `west_veh`) could help understand how pedestrian safety is affected by vehicle traffic direction and volume.
- **Distance from Downtown and Safety Measures:** The effect of an intersection's distance from downtown (`distdt`, `ln_distdt`) on the effectiveness of safety measures might vary, considering that downtown areas could have different traffic and pedestrian patterns.
- **Temporal Factors and Traffic Flow:** The interaction between temporal factors (`month`, `dow`) and traffic flow variables (fi, pi) might uncover seasonal or weekly patterns in pedestrian safety.

```{r}
# Create the initial model with extended interactions
initial_model <- lm(acc ~ . 

                    # Existing interactions with month and day of week
                    + month * cli
                    + month * cri
                    + month * cti
                    + month * ln_cli
                    + month * ln_cri
                    + month * ln_cti
                    + dow * cli
                    + dow * cri
                    + dow * cti
                    + dow * ln_cli
                    + dow * ln_cri
                    + dow * ln_cti

                    # Traffic Flow and Pedestrian Protection Measures
                    + fi * any_ped_pr
                    + pi * lt_protect
                    + fi * lt_restric
                    + pi * lt_prot_re
                    + fi * ped_countd
                    + pi * curb_exten

                    # Road Characteristics and Safety Measures
                    + median * any_ped_pr
                    + any_exclus * lt_protect
                    + tot_road_w * curb_exten

                    # Directional Traffic Flow with Specific Safety Measures
                    + north_veh * half_phase
                    + east_veh * ped_countd
                    + south_veh * green_stra
                    + west_veh * any_ped_pr

                    # Pedestrian and Vehicle Flow Interactions
                    + pi * fi
                    + north_ped * north_veh
                    + east_ped * east_veh
                    + south_ped * south_veh
                    + west_ped * west_veh

                    # Distance from Downtown and Safety Measures
                    + ln_distdt * any_ped_pr
                    + distdt * lt_protect

                    # Temporal Factors and Traffic Flow
                    + month * fi
                    + dow * pi

                    # Interactions with Polynomial Terms
                    + pi_squared * lt_protect
                    + fi_squared * any_ped_pr
                    + distdt_squared * green_stra
                    + distdt_cubed * half_phase
                    + tot_crossw_squared * lt_restric
                    + avg_crossw_squared * ped_countd
                    + tot_road_w_squared * curb_exten
                    + fli_squared * east_veh
                    + fri_squared * west_veh
                    + fti_squared * north_veh
                    
                    # Ignore the index int_no
                    - int_no 
                    
                    , data = dat_train)


# Perform stepwise feature selection with interactions using AIC
stepwise_aic <- stepAIC(initial_model, direction = "both", k = 2, trace=FALSE)

# Display the summary of the final model
summary(stepwise_aic)
```

```{r}
# Create a list object which will contain all the predictors for different methods 

# Extract the model formula
model_formula <- formula(stepwise_aic)

# Extract terms from the formula
model_terms <- labels(terms(model_formula))

# Since the first term is usually the response variable (left of ~), we remove it to get only predictors
selected_vars <- model_terms[-1] # Removes the first element, which is the response variable 'acc'

# Pack into a list under the key "stepwise_bic"
selected_covariates <- list(stepwise_aic = selected_vars)

# Print the list to see the selected variables
print(selected_covariates)
```
## Stepwise BIC 

```{r}
# Perform stepwise feature selection with interactions using BIC
stepwise_bic <- stepAIC(initial_model, direction = "both", k = log(nrow(dat_train)), trace=FALSE)

# Extract the model formula
model_formula <- formula(stepwise_bic)

# Extract terms from the formula
model_terms <- labels(terms(model_formula))

# Since the first term is usually the response variable (left of ~), we remove it to get only predictors
selected_vars <- model_terms[-1] # Removes the first element, which is the response variable 'acc'

# Pack into a list under the key "stepwise_bic"
selected_covariates$stepwise_bic <- selected_vars

# Print the list to see the selected variables
print(selected_covariates)
```

### Lasso 

```{r}
# Combine the response and predictor variables into a matrix for the training data
X_train <- model.matrix(acc ~ . - int_no, data = dat_dum_train)[, -1]  # Remove intercept column
y_train <- dat_dum_train$acc

# Set up a Lasso model with cross-validation on the training set
lasso_cv_model <- cv.glmnet(X_train, y_train, alpha = 1)  # alpha = 1 for Lasso

# # Plot the cross-validated mean squared error (CV MSE) as a function of log(lambda)
# plot(lasso_cv_model)

# Identify the lambda value that minimizes the CV MSE
best_lambda <- lasso_cv_model$lambda.min

# Display the selected lambda and the cross-validated mean squared error (CV MSE)
cat("Selected Lambda (lambda.min):", best_lambda, "\n")
cat("Cross-validated Mean Squared Error (CV MSE):", min(lasso_cv_model$cvm), "\n")

# Fit the final Lasso model with the selected lambda on the training set
final_lasso_model <- glmnet(X_train, y_train, alpha = 1, lambda = best_lambda)

# Extract coefficients from the final model
selected_features <- coef(final_lasso_model)

# Display the selected features
print(selected_features)
```

```{r}
# Extract the selected features from the Lasso model
selected_vars_lasso <- rownames(selected_features[selected_features[, 1] != 0, , drop = FALSE])[-1]

# Pack into a list under the key "lasso"
selected_covariates$lasso <- selected_vars_lasso
print(selected_covariates$lasso)
```

### Random Forest Importance 


```{r}
# Ensure that 'dat' contains only the variables in 'all_vars' plus the target variable 'acc'
predictors <- setdiff(colnames(dat), c("acc", "int_no"))

# Train the random forest model
set.seed(123)  # for reproducibility
rf_model <- randomForest(acc ~ . - int_no, data = dat_train, importance = TRUE)

# Extract variable importance
importance_rf <- importance(rf_model)

# Create a data frame of variables and their importance
variable_importance <- data.frame(Variable = rownames(importance_rf), 
                                  Importance = importance_rf[, "%IncMSE"])
variable_importance <- variable_importance[order(variable_importance$Importance, 
                                                 decreasing = TRUE), ]
# variable_importance$description <- sapply(variable_importance$Variable, 
#                                                 function(x) f_get_description(x, varnames_dict))
rownames(variable_importance) <- NULL

# Calculate the total importance and cumulative importance
total_importance <- sum(variable_importance$Importance)
variable_importance$CumulativeImportance <- cumsum(variable_importance$Importance) / total_importance

# Print the sorted variable importance
print(variable_importance)
```

In this step, the variable selection is all variables such that the cumulative importance is around 95%. This means that adding more variables will not add much to the model. 

```{r}
# Determine a cutoff for cumulative importance, e.g., 95%
cutoff_threshold <- 0.95

# Select variables with cumulative importance below the threshold
selected_variables <- variable_importance[variable_importance$CumulativeImportance <= cutoff_threshold, ]
selected_variables <- selected_variables$Variable

# Pack into a list under the key "rf_importance"
selected_covariates$rf_importance <- selected_variables

# Display selected variables
print(selected_covariates$rf_importance)
```

### Correlation Importance (numerical only)

```{r}
# subset only the correct numerical variables
numerical_dat <- dat_dum_train[, sapply(dat_dum_train, is.numeric)]
numerical_dat <- numerical_dat[, !names(numerical_dat) %in% c('int_no')]

# Compute correlations
correlations_with_acc <- f_compute_correlations(numerical_dat, "acc", standarize = FALSE)
correlations_with_acc <- correlations_with_acc[rownames(correlations_with_acc) != "acc", ]

# Convert row names to a column
correlations_with_acc <- correlations_with_acc %>% rownames_to_column(var = "variable")

# Identifying the top n most positively and negatively correlated variables
top_n = 50
top_positively_correlated <- correlations_with_acc %>% 
                             arrange(desc(spearman)) %>%
                             head(top_n+1)

top_negatively_correlated <- correlations_with_acc %>% 
                             arrange(spearman) %>%
                             head(top_n+1)


# Filter out rows with negative correlation in positive correlated, and vice versa
top_positively_correlated <- top_positively_correlated[top_positively_correlated$spearman > 0, ]
top_negatively_correlated <- top_negatively_correlated[top_negatively_correlated$spearman < 0, ]

# filter the target variable out of the correlations 
top_positively_correlated <- top_positively_correlated[-1, ]
top_negatively_correlated <- top_negatively_correlated[-1, ]

# reset the index of both corr tables
rownames(top_positively_correlated) <- seq(1, nrow(top_positively_correlated))
rownames(top_negatively_correlated) <- seq(1, nrow(top_negatively_correlated))

# Add a column containing the description of the variables
top_positively_correlated$description <- sapply(top_positively_correlated$variable, 
                                                function(x) f_get_description(x, varnames_dict))
top_negatively_correlated$description <- sapply(top_negatively_correlated$variable, 
                                                function(x) f_get_description(x, varnames_dict))

# Convert the description column to a character vector if it's not already
top_positively_correlated$description <- as.character(top_positively_correlated$description)
top_negatively_correlated$description <- as.character(top_negatively_correlated$description)

# Presenting the tables
print(top_positively_correlated)
print(top_negatively_correlated)
```

Finally,we perform variable selection with respect to the top correlated (in absolute value) variables which have at least 30% Spearman correlation with the target variable `acc`.  

```{r}
# Combine the positively and negatively correlated variables into one data frame
combined_correlations <- rbind(top_positively_correlated, top_negatively_correlated)

# Filter for variables with an absolute Spearman correlation of at least 30%
significant_correlations <- combined_correlations[abs(combined_correlations$spearman) >= 0.15, ]

# Extract the variable names into the selected_covariates list
selected_covariates$spearman <- significant_correlations$variable

# Print the selected covariates
print(selected_covariates)
```

# Prediction 

Based on the selected variables, we would like to select the "best" model by fitting a model again to the training data 


1. **Benchmark:** A model that predicts the mean of the target variable.
2. **Basic Linear Regression:** A simple linear regression model with no variable selection.
3. **Stepwise OLS:** A linear regression model with variable selection using stepwise regression.

1. **Stepwise variables:** Linear/Ridge Regression 
2. **Lasso variables:** Linear/Lasso Regression
3. **Random Forest Importance variables:** Random Forest Model
4. **Spearman Correlation variables:** Linear/Ridge Regression
5. **No selection:** This will be used for pure random forest on top. 

We will then compare the performance of the models on the validation set.

```{r}
# Create a dataframe to store performances
model_performance <- data.frame(
  Model_Name = character(), 
  MSE = numeric()  
)
```


## Benchmark 


```{r}
# Predicting the mean of acc which is very close to zero
mse = mean((mean(dat$acc) - dat_val$acc)^2)
model_performance <- rbind(model_performance, list(Model_Name = "Baseline", MSE = mse))
model_performance
```


## Basic Linear Regression with no variable selection

```{r}
# Fit linear regression model using all predictors
lm_model <- lm(acc ~ ., data = dat_train)

# Make predictions on the validation set
predictions <- predict(lm_model, newdata = dat_val)

# Calculate mean squared error
mse <- mean((predictions - dat_val$acc)^2)

# Print the mean squared error
print(paste("Mean Squared Error (MSE) on validation set:", mse))

# Store the model performance in dataset
model_performance <- rbind(model_performance, list(Model_Name = "OLS", MSE = mse))
```

```{r}
model_performance
```

## Stepwise AIC OLS

```{r}
# Test performance on validation set
predictions <- predict(stepwise_aic, newdata = dat_val)
actual <- dat_val$acc
mse <- mean((predictions - actual)^2)

print(paste("Mean Squared Error (MSE):", mse))

# Store the performance in dataset
model_performance <- rbind(model_performance, list(Model_Name = "Stepwise-AIC OLS", MSE = mse))
```

```{r}
model_performance
```


## Stepwise AIC OLS

```{r}
# Test performance on validation set
predictions <- predict(stepwise_bic, newdata = dat_val)
actual <- dat_val$acc
mse <- mean((predictions - actual)^2)

print(paste("Mean Squared Error (MSE):", mse))

# Store the performance in dataset
model_performance <- rbind(model_performance, list(Model_Name = "Stepwise-BIC OLS", MSE = mse))
```

```{r}
model_performance
```


## Stepwise-BIC and Lasso 

```{r}
# Create formula with interactions
interaction_formula <- as.formula(paste("acc ~", paste(selected_covariates$stepwise_bic, collapse = " + ")))

# Generate model matrix for train and val 
model_matrix_train <- model.matrix(interaction_formula, data = dat_train)
model_matrix_val <- model.matrix(interaction_formula, data = dat_val)

# Extract response variable
y <- dat_train$acc

# Fit Lasso model
lasso_model <- glmnet(model_matrix_train, y, alpha = 1)

# Make predictions on the validation set
predictions <- predict(lasso_model, newx =model_matrix_val)

# Calculate mean squared error
mse <- mean((predictions - dat_val$acc)^2)

# Print the mean squared error
print(paste("Mean Squared Error (MSE):", mse))

# Store the performance in dataset
model_performance <- rbind(model_performance, list(Model_Name = "Stepwise Lasso", MSE = mse))
```


```{r}
model_performance
```


## Lasso + OLS

Lasso + OLS 

```{r}
predictions <- predict(final_lasso_model, newx = model.matrix(acc ~ . - int_no, data = dat_val)[, -1], s = best_lambda)

mse <- mean((predictions - dat_val$acc)^2)

print(paste("Mean Squared Error (MSE):", mse))

# Store the performance in dataset
model_performance <- rbind(model_performance, list(Model_Name = "Simple Lasso", MSE = mse))
```


## Feature importance from RF

- RF Features + Ridge Model 

```{r}
X_train_rf <- as.matrix(dat_train[,selected_covariates$rf_importance])
ridge_model <- cv.glmnet(X_train_rf, y_train, alpha = 0)

# Print the selected lambda value
best_lambda <- ridge_model$lambda.min
cat("Selected lambda:", best_lambda, "\n")

# Fit the final Ridge regression model using the selected lambda
final_ridge_model <- glmnet(X_train_rf, y_train, alpha = 0, lambda = best_lambda)

# Print the final Ridge regression model
X_val <- as.matrix(dat_val[,selected_covariates$rf_importance])

# Make predictions using the fitted Ridge model
predictions <- predict(final_ridge_model, newx = X_val)

mse <- mean((predictions - dat_val$acc)^2)

print(paste("Mean Squared Error (MSE):", mse))

# Store the performance in dataset
model_performance <- rbind(model_performance, list(Model_Name = "Ridge+RF_feature", MSE = mse))
```

## Basic Random Forest

- No feature selection 
- NO hyperparm tuning 
- Just basics RF

```{r}
rf_model <- randomForest(acc ~ ., data = dat_train)

predictions <- predict(rf_model, newdata = dat_val)

mse <- mean((predictions - dat_val$acc)^2)

print(paste("Mean Squared Error (MSE):", mse))

# Store the performance in dataset
model_performance <- rbind(model_performance, list(Model_Name = "RandomForest", MSE = mse))
```

## Tuned RF

- Hyper tuned RF with basic features

```{r}
ctrl <- trainControl(method = "cv", number = 5, verbose=TRUE)

grid <- expand.grid(.mtry=c(5:7))

# Perform grid search
rf_model <- train(acc ~ ., data = dat_train, method = "rf",
                  trControl = ctrl, tuneGrid = grid,
                  nodesize = 5,ntree = 500)

plot(rf_model)

# Make predictions
predictions <- predict(rf_model, newdata = dat_val)
mse <- mean((predictions - dat_val$acc)^2)
print(mse)

model_performance <- rbind(model_performance, list(Model_Name = "Tuned RF", MSE = mse))
```
print(model_performance)


## Spearman Based Variables 

```{r}
lm_model <- lm(dat_train$acc ~ ., data =dat_train[,selected_covariates$spearman])
predictions <- predict(lm_model, newdata = dat_val)
mse <- mean((predictions - dat_val$acc)^2)

print(paste("Mean Squared Error (MSE) on validation set:", mse))

# Store the performance in dataset
model_performance <- rbind(model_performance, list(Model_Name = "OLS+Corr", MSE = mse))
```

## XGBoost 

### All performances 


```{r}
print(model_performance[order(-model_performance$MSE), ])
```



