---
title: "Road Safety Pilot Project - Variable Selection"
author: "Hair Albeiro Parra Barrera"
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO,CE]{Hair Parra}
- \fancyfoot[CO,CE]{Notes by Hair Parra}
- \fancyfoot[LE,RO]{\thepage}
output:
  html_document:
    df_print: paged
  pdf_document:
    extra_dependencies:
    - array
    - amsmath
    - booktabs
always_allow_html: true
geometry: margin=1.3cm
---

\newtheorem{assumption}{Assumption}[assumption]
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{remark*}{Remark}
\newtheorem{aside*}{Aside}
\newtheorem{exercise*}{Exercise}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=9, fig.height=6) 

# configurations for plot 
my_plot_hook <- function(x, options)
  paste("\n", knitr::hook_plot_tex(x, options), "\n")
knitr::knit_hooks$set(plot = my_plot_hook)
```

## Libraries 

```{r libraries, message=FALSE, warning=FALSE}
# CRAN libraries
library("sf") # temp
library("here")
library("tidyr")
library("dplyr")
library("tibble")
library("readxl")
library("corrplot")
library("lubridate")
library("ggmap") # temp 
library("ggplot2")
library("data.table")
library("MASS")
library(glmnet)
library(caret)

#install.packages("forcats")
library(forcats)


# Custom scripts  
source(here("functions", "utils.R"))
source(here("functions", "clean_data.R"))
source(here("functions", "install_packages.R"))

# load packages if necessary 
#f_load_packages()
```

## Load the data 


```{r message=FALSE, warning=FALSE}
# Load the dataset and perform data cleaning 
dat <- read.csv(here("data_raw", "data_final.csv"), sep=";")

dat <- f_clean_data(dat)

# Load variable descriptors 
result <- f_load_varnames(here("data_raw", "Dictionnaire_final.xlsx"))
varnames <- result$varnames
varnames_dict <- result$varnames_dict

# preview dat 
head(dat)

```
The variable 'all_pedest' has very rare occurence.

The variable 'number of' could be treated as categorical and is very unevenly distributed.

The variables traffic_10000 = fi
The variable ped_100 = pi

Visually, the ln transformation seem to make the relationship between predictors and target more linear.

```{r message=FALSE, warning=FALSE}
summary(dat)

table(dat$number_of_)

table(dat$all_pedest)

table(dat$borough_grouped)


# Function to plot numerical variables against the target variable
plotNumericalVsTarget <- function(input_df, target_variable) {
  numerical_columns <- sapply(input_df, is.numeric)
  
  for (col in names(input_df)[numerical_columns]) {
    if (col != target_variable) {
      print(col)
      c = ggplot(input_df, aes_string(x = col, y = target_variable)) +
        geom_point() +
        labs(title = paste(col, "vs", target_variable),
             x = col, y = target_variable)
      print(c)
    }
  }
}


plotNumericalVsTarget(dat, "acc")


```

```{r message=FALSE, warning=FALSE}

# Check the frequency of each level in the 'borough' factor variable
borough_levels <- table(dat$borough)

# Identify levels with fewer than 30 observations
levels_to_group <- names(borough_levels[borough_levels < 50])

# Create a new factor with 'Other' level for levels with fewer than 30 observations
dat$borough_grouped <- fct_collapse(dat$borough, Other = levels_to_group)

# Print the updated 'borough' factor variable
cat("Original 'borough' factor variable:\n")
table(dat$borough)

cat("\n'borough' factor variable with 'Other' level:\n")
table(dat$borough_grouped)

```


```{r message=FALSE, warning=FALSE}
# Create a linear regression model using caret and perform cross-validation
ctrl <- trainControl(method = "cv", number = 10)
model <- train(acc ~., data = dat, method = "lm", trControl = ctrl)

# Print the cross-validated results
print(model)

# Calculate the average MSE across all folds
average_mse <- mean(cv_mse, na.rm = TRUE)
cat("\nAverage MSE across all folds:", average_mse)
```



```{r message=FALSE, warning=FALSE}
# Create relevant interactions

initial_model <- lm(acc ~ . 
                    + month * cli
                    + month * cri
                    + month * cti
                    + month * ln_cli
                    + month * ln_cri
                    + month * ln_cti
                    + dow * cli
                    + dow * cri
                    + dow * cti
                    + dow * ln_cli
                    + dow * ln_cri
                    + dow * ln_cti
                    , data = dat)

#initial_model <- lm(acc ~ . , data = dat)

# Perform stepwise feature selection with interactions using AIC
final_model <- stepAIC(initial_model, direction = "both", k = log(nrow(dat)))

# Display the summary of the final model
summary(final_model)

# Best BIC found so far is 3576.66 using stepwise selection
# Final model:

#lm(formula = acc ~ x + fi + fri + cli + cti + ln_pi + ln_fri + 
#    ln_cri + ln_cti + tot_crossw + median + green_stra + half_phase + 
#    north_veh + commercial + ln_distdt + missing_date_ind + dow + 
#    parking + cti:dow, data = dat)
```
Performance measures collected were retrieved at a specfic time of the year. This may affect the impact of those variables. Create interaction.

```{r message=FALSE, warning=FALSE}
# Combine the response and predictor variables into a matrix
X <- model.matrix(acc ~ ., data = dat)[, -1]  # Remove intercept column
y <- dat$acc

# Set up a Lasso model with cross-validation
lasso_cv_model <- cv.glmnet(X, y, alpha = 1)  # alpha = 1 for Lasso

# Plot the cross-validated mean squared error (CV MSE) as a function of log(lambda)
plot(lasso_cv_model)

# Identify the lambda value that minimizes the CV MSE
best_lambda <- lasso_cv_model$lambda.min

cat("Selected Lambda (lambda.min):", lasso_cv_model$lambda.min, "\n")
cat("Cross-validated Mean Squared Error (CV MSE):", min(lasso_cv_model$cvm), "\n")

# Fit the final Lasso model with the selected lambda
final_lasso_model <- glmnet(X, y, alpha = 1, lambda = best_lambda)

# Extract coefficients from the final model
selected_features <- coef(final_lasso_model)

# Display the selected features
print(selected_features)

# MSE on cross validation is 6.7
```


