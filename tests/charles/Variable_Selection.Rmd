---
title: "Road Safety Pilot Project - Variable Selection"
author: "Hair Albeiro Parra Barrera"
header-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO,CE]{Hair Parra}
- \fancyfoot[CO,CE]{Notes by Hair Parra}
- \fancyfoot[LE,RO]{\thepage}
output:
  html_document:
    df_print: paged
  pdf_document:
    extra_dependencies:
    - array
    - amsmath
    - booktabs
always_allow_html: true
geometry: margin=1.3cm
---

\newtheorem{assumption}{Assumption}[assumption]
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{remark*}{Remark}
\newtheorem{aside*}{Aside}
\newtheorem{exercise*}{Exercise}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=9, fig.height=6) 

# configurations for plot 
my_plot_hook <- function(x, options)
  paste("\n", knitr::hook_plot_tex(x, options), "\n")
knitr::knit_hooks$set(plot = my_plot_hook)
```

## Libraries 

```{r libraries, message=FALSE, warning=FALSE}
# CRAN libraries
library("sf") # temp
library("here")
library("tidyr")
library("dplyr")
library("tibble")
library("readxl")
library("corrplot")
library("lubridate")
library("ggmap") # temp 
library("ggplot2")
library("data.table")
library("MASS")
library(glmnet)

# Custom scripts  
source(here("functions", "utils.R"))
source(here("functions", "clean_data.R"))
source(here("functions", "install_packages.R"))

# load packages if necessary 
#f_load_packages()
```

## Load the data 


```{r message=FALSE, warning=FALSE}
# Load the dataset and perform data cleaning 
dat <- read.csv(here("data_raw", "data_final.csv"), sep=";")

dat <- f_clean_data(dat)


# Load variable descriptors 
result <- f_load_varnames(here("data_raw", "Dictionnaire_final.xlsx"))
varnames <- result$varnames
varnames_dict <- result$varnames_dict

# preview dat 
head(dat)



```


```{r message=FALSE, warning=FALSE}
# Create relevant interactions

initial_model <- lm(acc ~ . 
                    + month * cli
                    + month * cri
                    + month * cti
                    + month * ln_cli
                    + month * ln_cri
                    + month * ln_cti
                    + dow * cli
                    + dow * cri
                    + dow * cti
                    + dow * ln_cli
                    + dow * ln_cri
                    + dow * ln_cti
                    , data = dat)

#initial_model <- lm(acc ~ . , data = dat)

# Perform stepwise feature selection with interactions using AIC
final_model <- stepAIC(initial_model, direction = "both", k = log(nrow(dat)))

# Display the summary of the final model
summary(final_model)

# Best BIC found so far is 3576.66 using stepwise selection

```
Performance measures collected were retrieved at a specfic time of the year. This may affect the impact of those variables. Create interaction.

```{r message=FALSE, warning=FALSE}
# Combine the response and predictor variables into a matrix
X <- model.matrix(acc ~ ., data = dat)[, -1]  # Remove intercept column
y <- dat$acc

# Set up a Lasso model with cross-validation
lasso_cv_model <- cv.glmnet(X, y, alpha = 1)  # alpha = 1 for Lasso

# Plot the cross-validated mean squared error (CV MSE) as a function of log(lambda)
plot(lasso_cv_model)

# Identify the lambda value that minimizes the CV MSE
best_lambda <- lasso_cv_model$lambda.min

cat("Selected Lambda (lambda.min):", lasso_cv_model$lambda.min, "\n")
cat("Cross-validated Mean Squared Error (CV MSE):", min(lasso_cv_model$cvm), "\n")

# Fit the final Lasso model with the selected lambda
final_lasso_model <- glmnet(X, y, alpha = 1, lambda = best_lambda)

# Extract coefficients from the final model
selected_features <- coef(final_lasso_model)

# Display the selected features
print(selected_features)

# MSE on cross validation is 6.7
```


